{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we go about training the agent with the MADDPG algorithm. To this end we import the required packages and load the enviromenent. Make sure that the Tennis app is in the local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from training import train\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from model import Actor\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can adjust hyperparameters of the training. It has the capability to do MADDPG or MAD4PG. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "param={    \n",
    "    'n_step':1, #how many time steps we take before we make our TD estimate\n",
    "    'n_episodes':10000, #maximum episodes to run (code will stop if average score is above 30 before this number)\n",
    "    'noise':True,# controls whether Ornsteinâ€“Uhlenbeck noise is added to each parameter of the actor\n",
    "    'print':False,\n",
    "    'agentparam':\n",
    "    {\n",
    "        'device':'cpu',\n",
    "        'action_size' : 2, \n",
    "        'state_size' : 24,\n",
    "        'seed' : 10,\n",
    "        'update_times':1, #how many times we learn at every step\n",
    "\n",
    "        'ddpgtype':'singleval', #distribution type: 'catdist' for categorical distribution (MAD4PG) or \"singleval\" for classic MADDPG\n",
    "        'atoms' : 51, #atoms for D4PG\n",
    "\n",
    "\n",
    "        'catparam':\n",
    "        {\n",
    "            'vmin' : 0, #minimum q category\n",
    "            'vmax' : 1 #maximum q category\n",
    "        },\n",
    "\n",
    "        'gamma' : 0.999, #discount factor : how much future rewards are discounted\n",
    "        'lr_actor' : 1e-4, #actor network learning rate\n",
    "        'lr_critic' : 1e-3, #critic network learning rate\n",
    "        'l2weights' : 0, #weight decay in adam optimizer, equivalent to l2 regularization\n",
    "        'tau' : 1e-3, #controls how quickly the target networks are updated\n",
    "        'explore_factor' :0.1, #controls how much noise is added to each action parameter\n",
    "        'batch_size' : 512,#how many experiences sampled in each batch\n",
    "\n",
    "        'replay_param' :\n",
    "        {\n",
    "            'buffer_size': int(1e6), #max length of buffer\n",
    "        },\n",
    "    \n",
    "        'OUparam':\n",
    "        {\n",
    "            'sigma': 0.5, #width of noise distribution\n",
    "            'mu':0, #Noise will be centered around mu\n",
    "            'theta':0.15 #how large a step we take towards the mean\n",
    "        },\n",
    "        \n",
    "        'criticparam':\n",
    "        {\n",
    "            'layer_units':[300,150] #List items create layers with n number of neurons, add item for additional layer\n",
    "        }, \n",
    "        \n",
    "        'actorparam':\n",
    "        {\n",
    "            'layer_units':[200,100]  #List items create layers with n number of neurons, add item for additional layer\n",
    "        }\n",
    "        \n",
    "\n",
    "    }\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.00\n",
      "Episode 200\tAverage Score: 0.01\n",
      "Episode 300\tAverage Score: 0.01\n",
      "Episode 400\tAverage Score: 0.03\n",
      "Episode 500\tAverage Score: 0.02\n",
      "Episode 600\tAverage Score: 0.00\n",
      "Episode 700\tAverage Score: 0.00\n",
      "Episode 800\tAverage Score: 0.01\n",
      "Episode 900\tAverage Score: 0.02\n",
      "Episode 1000\tAverage Score: 0.04\n",
      "Episode 1100\tAverage Score: 0.03\n",
      "Episode 1200\tAverage Score: 0.07\n",
      "Episode 1300\tAverage Score: 0.11\n",
      "Episode 1400\tAverage Score: 0.12\n",
      "Episode 1500\tAverage Score: 0.09\n",
      "Episode 1600\tAverage Score: 0.11\n",
      "Episode 1700\tAverage Score: 0.13\n",
      "Episode 1800\tAverage Score: 0.15\n",
      "Episode 1900\tAverage Score: 0.18\n",
      "Episode 2000\tAverage Score: 0.19\n",
      "Episode 2100\tAverage Score: 0.23\n",
      "Episode 2200\tAverage Score: 0.98\n",
      "Episode 2300\tAverage Score: 2.78\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultiAgent' object has no attribute 'actionestimator_local'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-af9a6d22400d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavgscores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/MachineLearning/deep-reinforcement-learning/p3_collab-compet/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, param)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi_episode\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mttime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactionestimator_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkpoint_actor.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQval_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkpoint_critic.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultiAgent' object has no attribute 'actionestimator_local'"
     ]
    }
   ],
   "source": [
    "scores,avgscores=train(env,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is considered solved at 1 and thus here it is solved. By running the following cell our results can be seen in tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.4.1 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph of score vs episode for the training session above is also presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"AverageScoreMADDPG.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='AverageScoreMADDPG.png')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load and test our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actor(\n",
       "  (hlayers): Sequential(\n",
       "    (0): Linear(in_features=24, out_features=200, bias=True)\n",
       "    (1): SELU()\n",
       "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (3): SELU()\n",
       "  )\n",
       "  (out): Linear(in_features=100, out_features=2, bias=True)\n",
       "  (outlayer): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=2, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainedActor0=Actor(24,2,10,param['agentparam']['actorparam'])\n",
    "TrainedActor0.load_state_dict(torch.load('checkpoint_actor0.pth'))\n",
    "TrainedActor0.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actor(\n",
       "  (hlayers): Sequential(\n",
       "    (0): Linear(in_features=24, out_features=200, bias=True)\n",
       "    (1): SELU()\n",
       "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (3): SELU()\n",
       "  )\n",
       "  (out): Linear(in_features=100, out_features=2, bias=True)\n",
       "  (outlayer): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=2, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainedActor1=Actor(24,2,10,param['agentparam']['actorparam'])\n",
    "TrainedActor1.load_state_dict(torch.load('checkpoint_actor1.pth'))\n",
    "TrainedActor1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors=[TrainedActor0,TrainedActor1]\n",
    "actions=np.zeros((2,2))\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 4.6000000685453415\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "num_agents=20                                          # change to appropriate number for your enviroment\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    states=torch.from_numpy(states).float()\n",
    "    for i,state in enumerate(states):\n",
    "        actions[i,:] = actors[i](state).detach().numpy()    # select an action (for each agent)\n",
    "        actions[i,:] = np.clip(actions[i,:], -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += np.max(env_info.rewards)                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"trainedTennis.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='trainedTennis.gif')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a gif of our agent getting the score of 4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future we could try a few things. I think implementing starting with a greater noise and exploration and then introducing a decay hyperparameter would help us find the right model parameters quickly and then converge well once we have reached found the first good area to search for neural network weights. In MADDPG it took us till around episode 1800 to find this area and this could help us find it sooner and solve our system with less training time. \n",
    "\n",
    "Hyperparameters could also be searched with a grid search, given enough training time to find the hyperparameters that could achieve a certain score with the least amount of training.\n",
    "\n",
    "Prioritized replay might also help with our sittuation at the beggining by selecting the most important experiences. It seemed once the model started getting enough of the right experiences it rapidly learned and so prioritized replay in particular has good potential too improve our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
