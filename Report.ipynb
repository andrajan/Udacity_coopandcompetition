{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we go about training the agent with the MADDPG algorithm. To this end we import the required packages and load the enviromenent. Make sure that the Tennis app is in the local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from training import train\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from model import Actor\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "DDPG is an extension of the DQN algorithm to continuous action spaces. When the environment contains a continuous action space we are unable to choose the maximum activon value for a given state by direct comparison as one might do in a discrete space. Therefore another set of neural networks is introduced to act as a action maximizer. We use this action to make take our steps in the enviroment and update the neural networks that give us our action values. We call the action value networks critics and the action networks actors. The critic network is updated by using a collected experience to find a difference between the Q value we get in one state and the value of that Q value that we would predict from another state. We take the gradient of this loss and update our network based on gradient descent. In order to avoid instability two different networks are used a target network and a local network. This is also done for the actor network. The local actor network is updated by sending the outputed action in combination with the state from an experience to the critc network and then using gradient ascent. We use gradient ascent because this is suppsoed to be an action maximizer.\n",
    "\n",
    "D4PG is almost the same but uses a distributional perspective for the Q value neural network. This was introduced by Barth-Maron et al (https://openreview.net/pdf?id=SyZipzbCb) where they found categorical distributions to work very well compared to standard DDPG. To calculate the loss in the case we take the cross entropy of the target and local distributions.\n",
    "\n",
    "Another modification available in the code here is n_step returns. Rather than training every step this trains every n steps. This can be seen as sortof half way towards monte carlo estimates. We also provide the oportunity to train the agent multiple times (in addition to the batch size) during this time.\n",
    "\n",
    "Noise can also be turned on to enhance exploration. This should help the agent adequately explore the space especially early in training. We also use a explore_factor that controls how much noise is added to the weights of the actor network.\n",
    "\n",
    "Rather than Relu activation functions our neural networks are built using SELU activation functions which should normalize our parameters in each layer to mean zero and standard deviation 1 as they train.\n",
    "\n",
    "MADDPG is an extension of DDPG for multiple cooperating agents. For each agent, MADDPG uses a critic which can see both agents observations and actions and an actor which can only see the local observations. MAD4PG uses D4PG instead of DDPG for the individual agents. This did not work as well as I hoped, maybe additional hyperparameter tunning is needed for the MAD4PG.\n",
    "\n",
    "below you can adjust the hyperparameters of the training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "param={    \n",
    "    'n_step':1, #how many time steps we take before we make our TD estimate\n",
    "    'n_episodes':10000, #maximum episodes to run (code will stop if average score is above 30 before this number)\n",
    "    'noise':True,# controls whether Ornsteinâ€“Uhlenbeck noise is added to each parameter of the actor\n",
    "    'print':False,\n",
    "    'agentparam':\n",
    "    {\n",
    "        'device':'cpu',\n",
    "        'action_size' : 2, \n",
    "        'state_size' : 24,\n",
    "        'seed' : 10,\n",
    "        'update_times':1, #how many times we learn at every step\n",
    "\n",
    "        'ddpgtype':'singleval', #distribution type: 'catdist' for categorical distribution (MAD4PG) or \"singleval\" for classic MADDPG\n",
    "        'atoms' : 51, #atoms for D4PG\n",
    "\n",
    "\n",
    "        'catparam':\n",
    "        {\n",
    "            'vmin' : 0, #minimum q category\n",
    "            'vmax' : 1 #maximum q category\n",
    "        },\n",
    "\n",
    "        'gamma' : 0.999, #discount factor : how much future rewards are discounted\n",
    "        'lr_actor' : 1e-4, #actor network learning rate\n",
    "        'lr_critic' : 1e-3, #critic network learning rate\n",
    "        'l2weights' : 0, #weight decay in adam optimizer, equivalent to l2 regularization\n",
    "        'tau' : 1e-3, #controls how quickly the target networks are updated\n",
    "        'explore_factor' :0.1, #controls how much noise is added to each action parameter\n",
    "        'batch_size' : 512,#how many experiences sampled in each batch\n",
    "\n",
    "        'replay_param' :\n",
    "        {\n",
    "            'buffer_size': int(1e6), #max length of buffer\n",
    "        },\n",
    "    \n",
    "        'OUparam':\n",
    "        {\n",
    "            'sigma': 0.5, #width of noise distribution\n",
    "            'mu':0, #Noise will be centered around mu\n",
    "            'theta':0.15 #how large a step we take towards the mean\n",
    "        },\n",
    "        \n",
    "        'criticparam':\n",
    "        {\n",
    "            'layer_units':[300,150] #List items create layers with n number of neurons, add item for additional layer\n",
    "        }, \n",
    "        \n",
    "        'actorparam':\n",
    "        {\n",
    "            'layer_units':[200,100]  #List items create layers with n number of neurons, add item for additional layer\n",
    "        }\n",
    "        \n",
    "\n",
    "    }\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.00\n",
      "Episode 200\tAverage Score: 0.01\n",
      "Episode 300\tAverage Score: 0.01\n",
      "Episode 400\tAverage Score: 0.03\n",
      "Episode 500\tAverage Score: 0.02\n",
      "Episode 600\tAverage Score: 0.00\n",
      "Episode 700\tAverage Score: 0.00\n",
      "Episode 800\tAverage Score: 0.01\n",
      "Episode 900\tAverage Score: 0.02\n",
      "Episode 1000\tAverage Score: 0.04\n",
      "Episode 1100\tAverage Score: 0.03\n",
      "Episode 1200\tAverage Score: 0.07\n",
      "Episode 1300\tAverage Score: 0.11\n",
      "Episode 1400\tAverage Score: 0.12\n",
      "Episode 1500\tAverage Score: 0.09\n",
      "Episode 1600\tAverage Score: 0.11\n",
      "Episode 1700\tAverage Score: 0.13\n",
      "Episode 1800\tAverage Score: 0.15\n",
      "Episode 1900\tAverage Score: 0.18\n",
      "Episode 2000\tAverage Score: 0.19\n",
      "Episode 2100\tAverage Score: 0.23\n",
      "Episode 2200\tAverage Score: 0.98\n",
      "Episode 2300\tAverage Score: 2.78\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultiAgent' object has no attribute 'actionestimator_local'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-af9a6d22400d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavgscores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/MachineLearning/deep-reinforcement-learning/p3_collab-compet/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, param)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi_episode\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mttime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactionestimator_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkpoint_actor.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQval_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkpoint_critic.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultiAgent' object has no attribute 'actionestimator_local'"
     ]
    }
   ],
   "source": [
    "scores,avgscores=train(env,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is considered solved at 0.5 and thus here it is solved at 2200 episodes. By running the following cell our results can be seen in tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.4.1 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph of score vs episode for the training session above is also presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"AverageScoreMADDPG.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='AverageScoreMADDPG.png')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "param['agentparam']['ddpgtype']='catdist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.00\n",
      "Episode 200\tAverage Score: 0.00\n",
      "Episode 300\tAverage Score: 0.00\n",
      "Episode 400\tAverage Score: 0.01\n",
      "Episode 500\tAverage Score: 0.02\n",
      "Episode 600\tAverage Score: 0.00\n",
      "Episode 700\tAverage Score: 0.00\n",
      "Episode 800\tAverage Score: 0.00\n",
      "Episode 900\tAverage Score: 0.02\n",
      "Episode 1000\tAverage Score: 0.01\n",
      "Episode 1100\tAverage Score: 0.02\n",
      "Episode 1200\tAverage Score: 0.00\n",
      "Episode 1300\tAverage Score: 0.00\n",
      "Episode 1400\tAverage Score: 0.00\n",
      "Episode 1500\tAverage Score: 0.00\n",
      "Episode 1600\tAverage Score: 0.00\n",
      "Episode 1700\tAverage Score: 0.00\n",
      "Episode 1800\tAverage Score: 0.00\n",
      "Episode 1900\tAverage Score: 0.00\n",
      "Episode 2000\tAverage Score: 0.00\n",
      "Episode 2100\tAverage Score: 0.00\n",
      "Episode 2200\tAverage Score: 0.00\n",
      "Episode 2300\tAverage Score: 0.00\n",
      "Episode 2400\tAverage Score: 0.00\n",
      "Episode 2500\tAverage Score: 0.00\n",
      "Episode 2600\tAverage Score: 0.00\n",
      "Episode 2700\tAverage Score: 0.00\n",
      "Episode 2800\tAverage Score: 0.00\n",
      "Episode 2900\tAverage Score: 0.00\n",
      "Episode 3000\tAverage Score: 0.00\n",
      "Episode 3100\tAverage Score: 0.00\n",
      "Episode 3200\tAverage Score: 0.00\n",
      "Episode 3300\tAverage Score: 0.00\n",
      "Episode 3400\tAverage Score: 0.00\n",
      "Episode 3500\tAverage Score: 0.00\n",
      "Episode 3600\tAverage Score: 0.00\n",
      "Episode 3700\tAverage Score: 0.00\n",
      "Episode 3800\tAverage Score: 0.00\n",
      "Episode 3900\tAverage Score: 0.00\n",
      "Episode 4000\tAverage Score: 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-af9a6d22400d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavgscores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/MachineLearning/deep-reinforcement-learning/p3_collab-compet/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, param)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mn_steps\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mculreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mculreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MachineLearning/deep-reinforcement-learning/p3_collab-compet/multiagent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done, n_step, logger)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_times\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mexp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MachineLearning/deep-reinforcement-learning/p3_collab-compet/multiagent.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences, n_step, logger)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m#backwards pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mQloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQval_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores,avgscores=train(env,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load and test our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actor(\n",
       "  (hlayers): Sequential(\n",
       "    (0): Linear(in_features=24, out_features=200, bias=True)\n",
       "    (1): SELU()\n",
       "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (3): SELU()\n",
       "  )\n",
       "  (out): Linear(in_features=100, out_features=2, bias=True)\n",
       "  (outlayer): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=2, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainedActor0=Actor(24,2,10,param['agentparam']['actorparam'])\n",
    "TrainedActor0.load_state_dict(torch.load('checkpoint_actor0.pth'))\n",
    "TrainedActor0.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actor(\n",
       "  (hlayers): Sequential(\n",
       "    (0): Linear(in_features=24, out_features=200, bias=True)\n",
       "    (1): SELU()\n",
       "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (3): SELU()\n",
       "  )\n",
       "  (out): Linear(in_features=100, out_features=2, bias=True)\n",
       "  (outlayer): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=2, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainedActor1=Actor(24,2,10,param['agentparam']['actorparam'])\n",
    "TrainedActor1.load_state_dict(torch.load('checkpoint_actor1.pth'))\n",
    "TrainedActor1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors=[TrainedActor0,TrainedActor1]\n",
    "actions=np.zeros((2,2))\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 4.6000000685453415\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "num_agents=20                                          # change to appropriate number for your enviroment\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    states=torch.from_numpy(states).float()\n",
    "    for i,state in enumerate(states):\n",
    "        actions[i,:] = actors[i](state).detach().numpy()    # select an action (for each agent)\n",
    "        actions[i,:] = np.clip(actions[i,:], -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += np.max(env_info.rewards)                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"trainedTennis.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='trainedTennis.gif')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a gif of our agent getting the score of 4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future we could try a few things. I think implementing starting with a greater noise and exploration and then introducing a decay hyperparameter would help us find the right model parameters quickly and then converge well once we have reached found the first good area to search for neural network weights. In MADDPG it took us till around episode 1800 to find this area and this could help us find it sooner and solve our system with less training time. \n",
    "\n",
    "Hyperparameters could also be searched with a grid search, given enough training time to find the hyperparameters that could achieve a certain score with the least amount of training.\n",
    "\n",
    "Prioritized replay might also help with our sittuation at the beggining by selecting the most important experiences. It seemed once the model started getting enough of the right experiences it rapidly learned and so prioritized replay in particular has good potential too improve our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
